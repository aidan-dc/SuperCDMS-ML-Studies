nohup: ignoring input
Model Name: Large DNN
Dataset: Reduced
Training Method: L1 Reg+Early Stopping
Batch Normalization: Yes

Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 85)                1700      
                                                                 
 batch_normalization (BatchN  (None, 85)               340       
 ormalization)                                                   
                                                                 
 leaky_re_lu (LeakyReLU)     (None, 85)                0         
                                                                 
 dense_1 (Dense)             (None, 100)               8600      
                                                                 
 batch_normalization_1 (Batc  (None, 100)              400       
 hNormalization)                                                 
                                                                 
 leaky_re_lu_1 (LeakyReLU)   (None, 100)               0         
                                                                 
 dense_2 (Dense)             (None, 50)                5050      
                                                                 
 batch_normalization_2 (Batc  (None, 50)               200       
 hNormalization)                                                 
                                                                 
 leaky_re_lu_2 (LeakyReLU)   (None, 50)                0         
                                                                 
 dense_3 (Dense)             (None, 25)                1275      
                                                                 
 batch_normalization_3 (Batc  (None, 25)               100       
 hNormalization)                                                 
                                                                 
 leaky_re_lu_3 (LeakyReLU)   (None, 25)                0         
                                                                 
 dense_4 (Dense)             (None, 10)                260       
                                                                 
 batch_normalization_4 (Batc  (None, 10)               40        
 hNormalization)                                                 
                                                                 
 leaky_re_lu_4 (LeakyReLU)   (None, 10)                0         
                                                                 
 dense_5 (Dense)             (None, 1)                 11        
                                                                 
=================================================================
Total params: 17,976
Trainable params: 17,436
Non-trainable params: 540
_________________________________________________________________
None

6.053572654724121 6.088324546813965 8.142898559570312
4.507083415985107 4.529918670654297 7.125219821929932
11.118246078491211 11.107807159423828 14.70682430267334
19.748201370239258 19.730728149414062 25.599105834960938
7.791557312011719 7.808337688446045 11.125811576843262
8.967548370361328 8.957916259765625 12.656688690185547
9.664655685424805 9.67834758758545 13.42878532409668
7.014151573181152 7.007448196411133 10.380020141601562
9.929512023925781 9.916855812072754 13.918976783752441
5.14639949798584 5.159788608551025 8.267963409423828
12.894647598266602 12.893445014953613 17.412647247314453
6.354167461395264 6.355917930603027 9.344232559204102
8.545244216918945 8.540565490722656 12.237524032592773
10.16434097290039 10.158418655395508 13.939311981201172
9.947771072387695 9.966680526733398 13.646598815917969
7.767441272735596 7.7730231285095215 10.967671394348145
4.035614967346191 4.04949426651001 6.062890529632568
12.089376449584961 12.079023361206055 15.83954906463623
8.551592826843262 8.539023399353027 12.587639808654785
9.944451332092285 9.967824935913086 12.910480499267578
8.546823501586914 8.585150718688965 11.268911361694336
7.786448001861572 7.7930803298950195 11.160390853881836
10.51901626586914 10.530654907226562 14.2144136428833
6.272974014282227 6.258997440338135 9.383914947509766
5.248965740203857 5.231768608093262 8.049002647399902
9.451558113098145 9.443503379821777 13.65709114074707
6.758767127990723 6.734908580780029 10.193603515625
7.712294578552246 7.728018283843994 10.922606468200684
9.045104026794434 9.044425964355469 12.852617263793945
7.983608245849609 7.9767374992370605 11.538565635681152
8.997573852539062 8.980873107910156 12.54177474975586
7.309230804443359 7.316374778747559 10.214899063110352
9.062480926513672 9.056414604187012 12.685051918029785
6.934244632720947 6.9627532958984375 8.639383316040039
3.8877322673797607 3.8912603855133057 6.34958553314209
8.670811653137207 8.670085906982422 12.299718856811523
8.503547668457031 8.518747329711914 12.358352661132812
5.982500076293945 5.976565361022949 9.405887603759766
12.904313087463379 12.893024444580078 17.426973342895508
5.5819621086120605 5.591861724853516 8.402612686157227
7.623622417449951 7.628620147705078 10.712559700012207
8.496341705322266 8.477219581604004 12.295376777648926
8.906648635864258 8.908750534057617 12.753437042236328
13.765449523925781 13.749783515930176 18.826141357421875
6.3107500076293945 6.299758434295654 9.960821151733398
7.393043518066406 7.384115219116211 10.740514755249023
5.96128511428833 5.940438747406006 8.841015815734863
7.609224319458008 7.595999240875244 10.98715877532959
12.71059799194336 12.699434280395508 17.244840621948242
5.238733291625977 5.238475322723389 8.557772636413574
